# 优化器总结
深度学习可以归结为一个优化问题，最小化目标函数 ；最优化的求解过程，首先求解目标函数的梯度  ，然后将参数 向负梯度方向更新,为学习率，表明梯度更新的步伐大小。最优化的过程依赖的算法称为优化器，可以看出深度学习优化器的两个核心是梯度与学习率，前者决定参数更新的方向后者决定参数更新程度。深度学习优化器之所以采用梯度是因为，对于高维的函数其更高阶导的计算复杂度大，应用到深度学习的优化中不实际。

　　深度学习的优化器有许多种类，学术界的研究也一直十分活跃，个人理解优化器可以分为两个大类。第一类是优化过程中，学习率 [公式] 不受梯度 [公式] 影响，全程不变或者按照一定的learning schedule随时间变化，这类包括最常见的SGD(随机梯度下降法)，带Momentum的SGD，带Nesterov的SGD，这一类可以叫作SGD系列；另一类是优化过程中，学习率随着梯度 [公式] 自适应的改变，并尽可能去消除给定的全局学习率的影响，这一类优化器有很多，常见的包括Adagrad Adadelta RMSprop Adam； 还有AdaMax Nadam Adamax NadamAMSgrad(ICLR 2018 best paper)，以及最近比较火的Adabound，这一系列可以称为自适应学习率系列。

## SGD

SGD为随机梯度下降,每一次迭代计算数据集的mini-batch的梯度,然后对参数进行跟新。

## momentum

Momentum参考了物理中动量的概念,前几次的梯度也会参与到当前的计算中,但是前几轮的梯度叠加在当前计算中会有一定的衰减。

## adagard

Adagard在训练的过程中可以自动变更学习的速率,设置一个全局的学习率,而实际的学习率与以往的参数模和的开方成反比。
## Adam

Adam利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率,在经过偏置的校正后,每一次迭代后的学习率都有个确定的范围,使得参数较为平稳。