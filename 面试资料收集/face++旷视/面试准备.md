旷视面试准备
=====================


知乎帖子
-----------------

整个面试一个小时，开始会让你自我介绍

1.介绍项目相关，一般是和你所面试的课题组或CV方向相关的项目经历会重点提问，询问项目的应用场景以及你所负责的部分（原理必须搞清 具体训练过程 在调参中你所用到的trick 数据预处理方面）

2.softmax的数学公式 包括i是什么 j是什么 exp过大时怎么处理，输入什么经过softmax层后得到什么输出（笔者对于这个输入输出有些模糊，当时只答出了输入一个vector得到一个和为1的向量，但感觉说的过于简单）

[知乎softmax讲解](https://www.zhihu.com/question/23765351)

3.卷积向前传播过程 数学计算 详细到权重

这个问的是特别特别细，笔者面试前自以为自己对CNN传播过程了解很透彻，但现场还是被问住了，建议参考cs231n反向传播一章，讲解很清楚。

4.梯度弥散问题的原因 解决办法（解决办法笔者只说出了选择更好的激活函数，比如relu leakrelu 但其实解决办法有很多 包括梯度剪切、正则 batchnorm 残差结构 LSTM等）

5.sigmoid函数及其相关性质，为什么导数的最大值为0.25

>个人解释:sigmoid函数非线性,如果不用该函数,那么深度神经网络基本退化成了一个MLP多层感知机,全是线性运算,对与高维度,非线性的分类不能很好的去拟合.失去了神经网络的意义.

>此处知识点,对比下相关的激活函数.sigmoid,relu,tanh
>--------------------------
> $$sigmiod(x)=1/(1+e^(-x))$$
>sigmoid函数的导数最大值为0.25,因为求导书之后为$m/((1+m)^2)$m取值范围为0到1,此函数的极值为0.25,在m=1取到,但是sigmoid函数有三个缺点:
> * 梯度容易消失
> * 梯度只能正向或者反向更新,收敛速度慢
> * 幂运算耗时
> 
> tanh函数(取值范围-1到1,解决了只能正向和反向传播问题,但是还是会梯度消失,并且幂运算慢
> $$ tanhx=(e^x-e^(-x))/((e^x+e^(-x)))$$
> relu函数
> $$relu=max(0,x)$$
> 解决梯度消失问题,收敛快,计算块,网络稀疏,缓解过拟合.
> 容易出现问题,网络参数初始化不好的话,导致部分参数永远不会更新.

6.深度学习初始化权重的问题
>首先如何初始化：
> * w全部初始化为0（不行，因为每一层输出一样，迭代一定次数基本上就不起作用了）
> * w随机初始化(最常用的办法，w初始化到一个较小范围，防止输出过大，sigmoid之后梯度太小，无法继续训练)
> * Xavier initialization输入输出尽量同分布，对于tanh激活较为友好，对relu不好
> * He initialization(何凯明提出)
> 
>初始化后特别重要
>
7.卷积运算的数学意义 物理意义
>信号与系统：

8.最后一道数据结构题，字符的全排列

总的来说，这次面试还是意识到自己的菜比，编程能力 姿势水平亟待提高，旷视还是一家很有科技氛围的公司，面试前在前台等待的时候碰巧遇见了孙剑大佬，也算是不虚此行吧。希望下次有机会再去旷视面试的时候能顺利拿到offer。




介绍项目和比赛：
1. 项目整体介绍：
2. 项目遇到那些bad case，怎么解决的
3. 扮演什么角色，怎么分配任务和工作。
   实习过程中的负责人，工作安排：
   首先是和54所的人确定项目的方案，进度以及定期开会汇报。还有和老板的
4. C++内容。
5. 数据怎么清洗的。
   
   主要是利用训练好的模型对训练集的测试，来分析测试结果。列出混淆矩阵，针对不在对角线上，占比较多的类别之间取检查数据标注情况，单独列出包含此类别的一些label图和原图的对比，检查是数据问题还是模型问题，如果是数据的话，那么重新修订标注规范。比如，我们在实验中遇到一个很大的问题，是遥感图像中的水和植被的识别，在训练过程中发现这两类有明显的混淆和错分情况，因为在标注的时候，水和植被颜色都较深，难以区分，那么后面我们根据遥感图像的特征，就是利用近红外信息，提取出ndvi指数的图像，红色是植被，黑色是水体，修改了数据集之后，水体和植被检测精度和iou明显提升。40，60————》60,90
6. 怎么解决数据不平衡的问题。
   我们数据的标注情况不是特别理想，数据类别不均衡，我们采用了随机裁剪中根据mask控制裁剪区域，尽可能多的取提取小类的样本数量，这个
7. 