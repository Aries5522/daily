旷视面试准备
=====================


知乎帖子
-----------------

整个面试一个小时，开始会让你自我介绍

1.介绍项目相关，一般是和你所面试的课题组或CV方向相关的项目经历会重点提问，询问项目的应用场景以及你所负责的部分（原理必须搞清 具体训练过程 在调参中你所用到的trick 数据预处理方面）

2.softmax的数学公式 包括i是什么 j是什么 exp过大时怎么处理，输入什么经过softmax层后得到什么输出（笔者对于这个输入输出有些模糊，当时只答出了输入一个vector得到一个和为1的向量，但感觉说的过于简单）

[知乎softmax讲解](https://www.zhihu.com/question/23765351)

3.卷积向前传播过程 数学计算 详细到权重

这个问的是特别特别细，笔者面试前自以为自己对CNN传播过程了解很透彻，但现场还是被问住了，建议参考cs231n反向传播一章，讲解很清楚。

4.梯度弥散问题的原因 解决办法（解决办法笔者只说出了选择更好的激活函数，比如relu leakrelu 但其实解决办法有很多 包括梯度剪切、正则 batchnorm 残差结构 LSTM等）

5.sigmoid函数及其相关性质，为什么导数的最大值为0.25

>个人解释:sigmoid函数非线性,如果不用该函数,那么深度神经网络基本退化成了一个MLP多层感知机,全是线性运算,对与高维度,非线性的分类不能很好的去拟合.失去了神经网络的意义.

>此处知识点,对比下相关的激活函数.sigmoid,relu,tanh
>--------------------------
> $$sigmiod(x)=1/(1+e^(-x))$$
>sigmoid函数的导数最大值为0.25,因为求导书之后为$m/((1+m)^2)$m取值范围为0到1,此函数的极值为0.25,在m=1取到,但是sigmoid函数有三个缺点:
> * 梯度容易消失
> * 梯度只能正向或者反向更新,收敛速度慢
> * 幂运算耗时
> 
> tanh函数(取值范围-1到1,解决了只能正向和反向传播问题,但是还是会梯度消失,并且幂运算慢
> $$ tanhx=(e^x-e^(-x))/((e^x+e^(-x)))$$
> relu函数
> $$relu=max(0,x)$$
> 解决梯度消失问题,收敛快,计算块,网络稀疏,缓解过拟合.
> 容易出现问题,网络参数初始化不好的话,导致部分参数永远不会更新.

6.深度学习初始化权重的问题
>首先如何初始化：
> * w全部初始化为0（不行，因为每一层输出一样，迭代一定次数基本上就不起作用了）
> * w随机初始化(最常用的办法，w初始化到一个较小范围，防止输出过大，sigmoid之后梯度太小，无法继续训练)
> * Xavier initialization输入输出尽量同分布，对于tanh激活较为友好，对relu不好
> * He initialization(何凯明提出)
> 
>初始化后特别重要
>
7.卷积运算的数学意义 物理意义
>信号与系统：

8.最后一道数据结构题，字符的全排列

总的来说，这次面试还是意识到自己的菜比，编程能力 姿势水平亟待提高，旷视还是一家很有科技氛围的公司，面试前在前台等待的时候碰巧遇见了孙剑大佬，也算是不虚此行吧。希望下次有机会再去旷视面试的时候能顺利拿到offer。

希望能帮助到看到这篇文章的各位！